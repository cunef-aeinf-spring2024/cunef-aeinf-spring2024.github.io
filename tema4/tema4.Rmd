---
title: "Tema 4"
author: "Roi Naveiro"
date: ""
output: 
   xaringan::moon_reader:
    css: "ds_slides.css"
    lib_dir: libs
    seal: false
    includes:
      after_body: insert-logo.html
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
---



```{r packages_setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
knitr::opts_chunk$set(fig.width=8, fig.height=6) 
```

class: center, middle, inverse, title-slide

<div class="title-logo"></div>

# Análisis y Explotación de la Información
 
## Tema 4 - Modelización

<br>
<br>
.pull-left[
### Roi Naveiro
]
---
## Modelización

Las herramientas de modelización tienen cuatro objetivos fundamentales:

* **Explorar** los datos: los modelos a veces revelan patrones que no son evidentes en
las visualizaciónes (>3D)

* **Generalizar** hallazgos de una muestra a la población (inferencia)

* Determinar **relaciones causa-efecto** (inferencia causal)


```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/data-science-model.png")
library(tidyverse)
library(patchwork)
```

---
## Modelización

Los patrones descubiertos por las herramientas de modelización pueden ser:

* Patrones de asociación
* Relaciones causa-efecto

Además, estos patrones pueden:

* Darse únicamente en los datos observados
* Generalizarse a la población 

---
## Modelización

La forma de recolectar los datos afecta al tipo de generalización de las 
conclusiones:

* Si queremos que las conclusiones extraídas a partir de una muestra de datos
sean generalizables a la población, debemos muestrear los sujetos **al azar**.

```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/bdl.png")
```

[Fuente](https://www.huffingtonpost.es/2016/03/28/troleo-en-la-red-para-bautizar-blas-de-lezo-a-un-buque-brita_n_9556114.html)

---
## Modelización

¿Muestrear datos al azar nos garantiza que los patrones detectados sean relaciones 
causa-efecto reales?

---
## Study: Cereal Keeps Girls Slim
 
Se observó que las mujeres que desayunaban tenían un **índice de masa corporal promedio más bajo**, un indicador común de obesidad, que las que no desayunaban. El índice fue aún más bajo para las que **desayunaban cereales**, según los hallazgos del estudio realizado por el Instituto de Investigación Médica de Maryland con fondos de NIH y el fabricante de cereales General Mills.

[...]

Los resultados se obtuvieron de una encuesta de NIH de 2379 mujeres en California, Ohio y Maryland.

[...]

Como parte de la encuesta, se preguntaba a las niñas una vez al año qué habían comido durante los tres días anteriores...

[Fuente](https://www.cbsnews.com/news/study-cereal-keeps-girls-slim/)

---
## Study: Cereal Keeps Girls Slim

Existen tres posibles explicaciones de este hallazgo:

* Comer cereales es la causa de que las mujeres estén delgadas

* Que las mujeres estén delgadas es la causa de que coman cerales

* Existe una tercera variable que es causa de estas dos, **variable de confusión**

Una **variable de confusión** es una variable exógena que es causa tanto a la variable explicativa como a la de respuesta, y que hace que parezca que existe una relación entre ellas.

---
## Study: Cereal Keeps Girls Slim

"Aquellos que desayunan regularmente tienen **más probabilidades de tener un plan de alimentación estructurado** a lo largo del día y, en consecuencia, es menos probable que coman entre comidas y consuman calorías vacías".

---
## Study: Cereal Keeps Girls Slim

¿Por qué se publica esto?

```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/follow-money.jpeg")
```

---
## Estudios científicos

Según el proceso de recolección de los datos, distinguimos estudios:

* **Observacionales**

  * Se recogen datos de forma que no se altera el proceso de generación de los mismos

  * Sirven para determinar **asociación**

* **Experimentales**

  * Se asignan diferentes tratamientos a distintos individuos 

  * Establecer relaciones **causa-efecto**

---
## Estudios científicos

```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/studies.png")
```

---
## Correlación no implica necesariamente causalidad

```{r, echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/margarina.png")
```

---
## Modelización

Las herramientas de modelización tienen tres objetivos fundamentales:

* **Explorar** los datos: los modelos a veces revelan patrones que no son evidentes en
las visualizaciónes (>3D)

* **Generalizar** hallazgos de una muestra a la población (inferencia)

* Determinar **relaciones causa-efecto** (inferencia causal)

```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/data-science-model.png")
```


---
class: center, middle, inverse

# Modelos para Análisis Exploratorio

---
## Modelización

* En lo que resta de curso, introduciremos herramientas básicas de modelización, poniendo
el foco en su uso como **técnicas de exploración** de datos.

* **No** vamos a estudiar cómo usar estas herramientas para determinar causalidad.

* No hay nada malo con la exploración, pero nunca debes vender un análisis exploratorio como un análisis confirmatorio porque es **engañoso**.

---
## ¿Qué son los modelos estadísticos?

* Herramientas que nos permiten extraer patrones de los datos.

* Patrones vs residuos

* Estudiaremos modelos que relacionan un serie de variables (variables predictoras)
coon una variable de interés (variable respuesta)

---
## ¿Qué son los modelos estadísticos?

Un modelo estadístico es un modelo matemático que incorpora un conjunto de supuestos estadísticos relacionados con la generación de datos de una muestra (observaciones) y datos similares de una población más grande. Un modelo estadístico representa, a menudo de forma idealizada, el proceso de generación de datos.

---
## ¿Qué son los modelos estadísticos?

* Formalmente, un modelo estadísticos es un par $(S \mathcal{P})$: $S$ es el conjunto de posibles observaciones y $\mathcal{P}$ es una familia de distribuciones de probabilidad.

* Asumimos que hay una distribución de probabilidad verdadera que gobierna la generación de datos.

* Se asume que la familia de distribuciones $\mathcal{P}$ contiene una distribución que aproxima de forma adecuada la distribución real.

* "A model is a simplification or approximation of reality and hence will not reflect all of reality."

---
## ¿Qué son los modelos estadísticos?

* Los modelos suelen ser paramétricos: d: $\mathcal {P}=\{F_{\theta }:\theta \in \Theta \}$


* Dados unos datos, el objetivo es encontrar el valor de $\theta$ que mejor se ajusta a los datos.


---
## ¿Qué son los modelos estadísticos?

* Nos vamos a centrar en modelos que relacionan una variable respuesta con una o más variables predictoras.

* Estos modelos se conocen como **modelos de regresión**.

* Empezemos con un ejemplo sencillo: la variable respuesta es continua y hay una sola variable predictora.

---
## ¿Qué son los modelos estadísticos?

```{r}
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() + theme_minimal() +
  geom_smooth(method="lm", se=FALSE, color='red')
```

---
## ¿Qué son los modelos estadísticos?


* En este caso $S = \mathbb{R}^2$. Una posibilidad para $\mathcal{P}$ es la familia de modelos lineales:
$$
\mathcal{P} = \{y \sim \mathcal{N}(\beta_0 + \beta_1 x, \sigma^2)\}
$$

* También podemos escribirlo como $y = \beta_0 + \beta_1 x + \epsilon$, donde $\epsilon \sim \mathcal{N}(0, \sigma^2)$

* Este modelo se conoce como **modelo de regresión lineal simple**.

* ¿Cuáles son los parámetros de este modelo?

---
## Regresión lineal simple

* Una vez observamos datos $(x_i, y_i) \in \mathcal{S}$, podemos ajustar el modelo para encontrar los valores de los parámetros que mejor se ajustan a los datos.

* El modelo resultante se denomina **modelo ajustado**.


**OJO**: el mejor modelo de la familia no tiene por qué ser la realidad. Los modelos son
simplificaciones de la realidad que nos sirven de algún propósito

---
## Regresión lineal simple

```{r, echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/box.jpeg")
```

---
## Vocabulario

* **Variable respuesta**: Variable cuyo comportamiento o variación se está tratando de entender. También llamada variable dependiente. Eje y.

* **Variables explicativas**: otras variables que desea utilizar para explicar la variación en la respuesta. También llamadas variables independientes, covariables, predictores o *features*. Eje x.

* **Valor predicho**: output del modelo para cierto valor de las covariables.

---
## Vocabulario

Discute los elementos anteriores en este ejemplo.

```{r}
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() + theme_minimal() +
  geom_smooth(method="lm", se=FALSE, color='red')
```

---
## Ajustando modelos

```{r}
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() + theme_minimal()
```

---
## Ajustando modelos

* Los datos anteriores presentan un patrón claro

* Usaremos un modelo para capturar ese patrón y hacerlo explícito

* Un modelo lineal parece razonable $y = \beta_0 + \beta_1 x + \epsilon$ con $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

* Vamos a asumir que $\sigma^2$ es conocido.

* Existen infinitos modelos en esta familia

---
## Ajustando modelos

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(mtcars, aes(wt, mpg)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point() + theme_minimal()
```

---
## Ajustando modelos

* La mayoría de estos modelos son **malos**, no capturan el patrón

* Necesitamos determinar qué modelos son **más cercanos** a los datos

* Una opción, usar el modelo que minimice suma de las distancias verticales de cada
punto a la recta del modelo

---
## Ajustando modelos

```{r, echo=F}
fit <- lm(mpg ~ wt, data = mtcars)
mtcars[,"predicted"] <- predict(fit)

ggplot(mtcars, aes(wt, mpg)) + geom_smooth(method="lm", se=F, color="red") +
  geom_segment(aes(xend = wt, yend = predicted), alpha = .2) +  # alpha 
  geom_point() + theme_minimal()
```


---
## Regresión lineal simple

Para obtener el valor de los coeficientes usando R, hacemos

```{r}
reg_mod <- lm(mpg ~ wt, data = mtcars)
coef(reg_mod)
```

* El objeto `mpg ~ wt` es una fórmula. Equivale a 

$$
\text{mpg} = \beta_0 + \beta_1 \cdot \text{wt}
$$

* Intercept es la estimación del coeficiente $\beta_0$ y el otro número es la estimación de $\beta_1$


---
## Regresión lineal

* Para estimar los valores de $\beta_0$ y $\beta_1$, usamos los datos.

* Llamamos a las estimaciones $\hat{\beta}_0$ y $\hat{\beta}_1$.

* Llamamos valor predicho a $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$.

* $\hat{\beta}_0$ y $\hat{\beta}_1$ son aquellos valores que hacen que las distancias verticales vistas anteriormente sean mínimas

$$
\arg \min_{\beta_0, \beta_1} \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_i)]^2
$$
---
## Regresión lineal

Demostrar que los valores que minimizan la expresión anterior son los estimadores de maximá verosimilitud de $\beta_0$ y $\beta_1$.

---
## Regresión lineal

* Los estimadores $\hat{\beta}_0$ y $\hat{\beta}_1$ tienen una distribución.

* Podemos estudiar la distribución de estos estimadores para obtener sus propiedades: sesgo, varianza, etc.

* Así como calcular intervalos de confianza para estos estimadores.

* En este curso no nos centraremos en este aspecto. Únicamente nos centraremos en cómo usar estos modelos para explorar los datos.

---
## Regresión lineal - Residuos

Los residuos nos dicen cómo de lejos está cada valor predicho de su valore observado

$e_i = y_i - \hat{y}_i$

* Los estimadores $\hat{\beta}_0$ y $\hat{\beta}_1$ son aquellos valores que minimizan la suma de los residuos al cuadrado.

---
## Visualización de modelos

Podemos visualizar:

* Las predicciones de los modelos

* Los residuos de los modelos

Para esto, utilizamos `modelr` de `tidyverse`

---
## Visualización de modelos

* Primero creamos una red de puntos de la variable predictora

```{r}
library(modelr)
grid <- mtcars %>% 
  data_grid(wt) 

grid
```

---
## Visualización de modelos

* Con el modelo, podemos añadir predicciones

```{r}
reg_mod <- lm(mpg ~ wt, data = mtcars)

grid <- grid %>% 
  add_predictions(reg_mod)

grid
```

---
## Visualización de modelos

```{r}
ggplot(mtcars, aes(wt)) +
  geom_point(aes(y = mpg)) + theme_minimal() +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)
```


---
## Visualización de modelos

Añadimos residuos usando

```{r}
reg_mod <- lm(mpg ~ wt, data = mtcars)

mtcars <- mtcars %>% add_residuals(reg_mod)
```

---
## Visualización de modelos

```{r}
ggplot(mtcars, aes(wt, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 
```

---
## Visualización de modelos

* Las predicciones nos dicen qué patrón hemos capturado

* Los residuos indican qué patrón queda sin capturar

---
## ¿Qué hacer con los modelos?

* **Explicación**: caracterizar la relación entre $y$ y $x$ a través de los parámetros
$\beta_0$ y $\beta_1$

* **Predicción**: para un nuevo valor de $x$, obtener su valor $y$

---
## Interpretación de la regresión lineal

Volvamos a la regresión anterior. La librería `broom` de tidyverse, sirve para 
ordenar los resultados de `lm`. **OJO**: no se carga automáticamente al cargar `tidyverse`.

```{r}
library(broom)
mod_reg <- lm(mpg ~ wt, data=mtcars)
tidy(mod_reg)
```

---
## Interpretación de la regresión lineal - Pendiente

El modelo de regresión lineal es

$$
\widehat{ \text{mpg} } = \beta_0 + \beta_1 \text{wt}
$$

Aumentemos wt en una unidad

\begin{eqnarray}
&& \beta_0 + \beta_1 ( \text{wt} + 1) = \\
&& \beta_0 + \beta_1 \text{wt} + \beta_1 = \\
&& \widehat{ \text{mpg} } + \beta_1
\end{eqnarray}

¿Cómo se interpreta $\beta_1$?

---
## Interpretación de la regresión lineal - Ordenada en el orign

El modelo de regresión lineal es

$$
\widehat{ \text{mpg} } = \beta_0 + \beta_1 \text{wt}
$$

Sustituímos wt por cero

\begin{eqnarray}
\widehat{ \text{mpg} } = \beta_0 + \beta_1 \times 0 = \beta_0
\end{eqnarray}

¿Cómo se interpreta $\beta_0$?

---
## Interpretación de la Regresión Lineal

Vamos a modelizar cómo afectan los años de experiencia en el salario de un profesor

```{r}
library(openintro)
data("teacher")
set.seed(1)
grade <- sample(c(rep("elementary", 20), rep("middle", 25), rep("high", 26)))
teacher <- teacher %>%
  mutate(degree = factor(degree, c("MA","BA")),
         grade = grade)
```

---
## Interpretación de la Regresión Lineal

Vamos a modelizar cómo afectan los años de experiencia en el salario de un profesor

```{r}
ggplot(teacher, aes(x=years, y=base)) + geom_point() + theme_minimal()
```

---
## Interpretación de la Regresión Lineal

Ajusta un modelo lineal, obtén los coeficientes e interprétalos.

Determina la predicción del salario de un profesor con 15 años de experiencia.

---
## Interpretación de la Regresión Lineal

```{r}
ggplot(teacher, aes(x=years, y=base)) + geom_point() +
  geom_smooth(method="lm", color='red', se=FALSE) + theme_minimal()
```


---
## Regresión Lineal: Predictores Categóricos

* Hasta ahora hemos considerado la $x$ contínua. ¿Qué sucede si es categórica?

* Imaginemos que la $x$ se refiere a género y toma dos valores: masculino y femenino.

* $y = \beta_0 + \beta_1 x$ no tendría sentido, $x$ no es un número!

* Podemos hacer $y = \beta_0 +\beta_1 \text{gen}\_\text{masc}$, donde $\text{gen}\_\text{masc}$ toma valor 1 para hombres y cero para mujeres

---
## Regresión Lineal: Predictores Categóricos

* Salario frente a grado. ¿Qué niveles tiene la variable degree?

* Ajustamos un modelo

```{r}
mod_base_degree <- lm(base ~ degree, data=teacher)
tidy(mod_base_degree)
```

---
## Regresión Lineal: Predictores Categóricos

* R ha creado una **variable indicatriz**  degreeBA: si el grado es BA toma valor 1,
sino 0.

* Si la variable tuviese tres niveles A, B y C, R crearía dos variables indicatrices, e.g. para A y B.

* El nivel base es el nivel que toma la variable cuando todas las indicatrices son 0.

* Los coeficientes se interpretan respecto al nivel base.


---
## Regresión Lineal: Predictores Categóricos

```{r}
mod_base_degree <- lm(base ~ degree, data=teacher)
tidy(mod_base_degree)
```
Interpreta cada coeficiente

---
## Regresión Lineal: Visualizamos las predicciones

```{r}
mod_base_degree <- lm(base ~ degree, data=teacher)

grid <- teacher %>% 
  data_grid(degree) %>% 
  add_predictions(mod_base_degree)
grid
```

---
## Regresión Lineal: Visualizamos las predicciones

```{r}
ggplot(teacher, aes(x=degree)) + geom_point(aes(y=base)) + theme_minimal() +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

---
## Regresión Lineal: Visualizamos las predicciones

Predecimos la media para cada grupo!

---
## Regresión Lineal Múltiple

* En múltiples ocasiones, tenemos más de una variable predictora.

* El modelo lineal más sencillo para este caso, es la **regresión lineal múltiple**

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
$$

* El efecto de cada variable se estima independientemente del resto de variables.

---
## Regresión Lineal Múltiple - Interpretación

* ¿Cómo se interpreta $\beta_0$? 

* ¿Cómo se interpreta $\beta_1$? 

---
## Regresión Lineal Múltiple 

En R

```{r}
mod_reg <- lm(base ~ years + degree, data = teacher)
tidy(mod_reg)
```

---
## Regresión Lineal Múltiple 

Visualizamos las predicciones

```{r}
grid <- teacher %>% 
  data_grid(years, degree) %>% 
  add_predictions(mod_reg)
grid
```

---
## Regresión Lineal Múltiple 

Visualizamos las predicciones

```{r}
ggplot(teacher, aes(years, base, colour = degree)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + theme_minimal()

```

---
class: center, middle, inverse

# Inferencia 

---

## Terminología

.vocab[Población]: grupo de individuos a estudiar

.vocab[Parámetro]: cantidad numérica derivada de la población
(casi siempre desconocida)

Si tuviéramos datos de cada unidad de la población, podríamos simplemente calcular
el parámetro de población y ¡listo!

--

**Desafortunadamente, en general no podemos hacer esto. 
En cambio, trabajamos con**

.vocab[Muestra]: un subconjunto de nuestra población de interés

.vocab[Estadístico]: una cantidad numérica derivada de una muestra


---

## Inferencia

Si la muestra es .vocab[representativa], entonces podemos utilizar 
la probabilidad y la inferencia estadística para obtener
conclusiones .vocab[generalizables] a la población de interés.



---

## Inferencia estadística

La .vocab[inferencia estadística] es el proceso que permite, a través de
datos de una muestra, obtener conclusiones sobre la población de la cual procede
la misma.

- .vocab[Estimación]: utilizar la muestra para estimar un rango plausible de 
valores de un parámetro poblacional desconocido

- .vocab[Contraste de hipótesis]: evaluar si la muestra observada proporciona evidencia 
a favor o en contra de alguna afirmación sobre la población

Nos enfocaremos en la **estimación**.

---

class: center, middle

# Estimación

---

## Viajamos a Asheville! 

```{r echo=FALSE,out.width=450, fig.align="center"}
knitr::include_graphics("img/asheville.jpg")
```

**¿Cuánto deberíamos esperar pagar por un Airbnb en Asheville?**


---

## Datos de Asheville

[Inside Airbnb](http://insideairbnb.com/) extrajo todos los listados de Airbnb en 
Asheville, NC, que estaban activos el 25 de junio de 2020.

**Población de interés**: airbnbs en Asheville con al menos diez reseñas.

**Parámetro de interés**: Precio promedio por huésped por noche en estos 
Airbnbs.


.pregunta[
¿Cuál es el precio promedio por huésped por noche de los alquileres de Airbnb en junio de 2020, 
con al menos diez reseñas en Asheville (códigos postales 28801 - 28806)?
]

Tenemos datos sobre el precio por huésped (`ppg`) para una muestra aleatoria de 50 listados de Airbnb.


---

## Estimación puntual

Un .vocab[estimador puntual] es un solo valor calculado a partir de los datos de 
la muestra para servir como la "mejor estimación" para el parámetro poblacional.

Nuestro estimador puntual para el precio promedio por huésped en la población es 
la media observada en la muestra:

```{r}
abb <- read_csv("data/asheville.csv")
abb %>% 
  summarize(mean_price = mean(ppg))
```

---

## Visualización de nuestra muestra

```{r, echo = F, out.width = "80%"}
ggplot(data = abb, aes(ppg)) +
  geom_histogram(binwidth = 25,
                 color = "darkblue",
                 fill = "skyblue") +
  labs(title = "Distribución sesgada a la derecha del precio por huésped",
       x = "Precio por huésped por noche ($)",
       y = "Recuento") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  theme_minimal()
```

---

## Estimador puntual?

.pregunta[
Si quieres estimar un parámetro de la población, ¿prefieres informar un rango de valores en los que podría estar el parámetro, o un solo valor?
]


--

- Si únicamente aportamos un estimador puntual, probablemente no acertemos 
exactamente el valor real del parámetro de la población.

- Si aportamos un rango de valores plausibles,
tenemos mayor probabilidad de capturar el valor real parámetro.


---

class: middle, center

## Intervalos de confianza

---

## Variabilidad de los estadísticos muestrales

Para construír un intervalo de confianza para la media de la población, 
necesitamos crear un rango de valores plausibles alrededor de la media muestral observada.

- Recuerda que las muestras aleatorias pueden diferir entre sí. Si tomamos otra
muestra aleatoria de 50 listados de Airbnb en Asheville, probablemente no obtendríamos la misma
media de precio por huésped.

--

- Existe cierta .vocab[variabilidad] en la media muestral.

- Para construir un intervalo de confianza, debemos cuantificar esta variabilidad. Esto
nos da una medida de cuánto esperamos que varíe la media muestral de
muestra a muestra.

---

## Variabilidad de los estadísticos muestrales

.question[
Supongamos que dividimos la clase por la mitad y le preguntamos a cada estudiante su altura. 
Luego, calculamos la altura media de los estudiantes
en cada lado del salón. ¿Esperarías que estas dos medias sean exactamente
iguales, cercanas pero no iguales, o muy diferentes?
]

--

<br><br>

.question[
Supongamos que tomas al azar 50 estudiantes y 5 de ellos son zurdos. Si
tomaras otra muestra aleatoria de 50 estudiantes, ¿cuántos esperarías que fueran
zurdos? ¿Te sorprendería si solo 3 de ellos fueran zurdos? ¿Te sorprendería si
40 de ellos fueran zurdos?
]

---

## Cuantificación de la variabilidad

Podemos cuantificar la variabilidad de los estadísticos muestrales mediante enfoques diferentes:

- **Simulación**: mediante técnicas como "bootstrap" o "remuestreo" (**lo que haremos aquí**)

o

- **Teoría**: mediante el Teorema del Límite Central

---

class: center, middle

# Bootstrapping

---

## El principio del bootstrap

<img src="img/boot.png" style="float:right">

- El término .vocab[bootstrap] proviene de la frase "levantarse a uno mismo 
tirando de la hebilla de las 
botas", que es una metáfora para lograr una tarea imposible sin 
ninguna ayuda externa.

- En este caso, la tarea imposible es estimar un parámetro de la población, y lo 
lograremos utilizando datos solo de la muestra dada.

- Esta noción de decir algo sobre un parámetro de la población usando 
solo información de una muestra observada es la esencia de la inferencia estadística;
no se limita al bootstrap.

---

## El procedimiento del bootstrap

1. Tomar una .vocab[muestra bootstrap] - una muestra aleatoria tomada **con reemplazo**
de la muestra original, **del mismo tamaño** que la muestra original.

2. Calcular el .vocab[estadístico bootstrap]:  el estadístico de interés (la 
media, la mediana, la correlación, etc.) usando la muestra bootstrap.

3. Repetir los pasos (1) y (2) muchas veces para crear una .vocab[distribución bootstrap] - una distribución de estadísticos bootstrap.

4. Calcular los límites del intervalo de confianza del XX% como la región que alberga el
XX% de la distribución bootstrap.

---

## La muestra original

<br><br>

```{r, echo = F, out.width = "70%"}
mean0 <- round(mean(abb$ppg),3)
ggplot(data = abb, aes(ppg)) +
  geom_histogram(binwidth = 25,
                 color = "darkblue",
                 fill = "skyblue") +
  labs(caption = paste0("Original sample: mean = ", mean0),
       x = "Price per guest per night",
       y = "Count") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(abb$ppg), lwd = 2, color = "red") + 
  theme_minimal()
```

---

## Paso a paso

**Paso 1.** Tomar una .vocab[muestra bootstrap]: una muestra aleatoria tomada 
**con reemplazamiento** de la muestra original, **del mismo tamaño** que la 
muestra original:


```{r, echo = F, out.width = "70%"}
set.seed(1)
indices <- sample(1:nrow(abb), 50, replace = T)
boot_samp_1 <- abb %>% 
  slice(indices)
```

```{r, echo = F, out.width = "70%"}
ggplot(data = boot_samp_1, aes(ppg)) +
  geom_histogram(binwidth = 25,
                 color = "darkblue",
                 fill = "skyblue") +
  labs(caption = "Bootstrap sample 1",
       x = "Price per guest per night",
       y = "Count") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
# geom_vline(xintercept = mean(boot_samp_1$ppg), lwd = 2, color = "red") + 
  theme_minimal()
```


---

## Paso a paso

**Paso 2.** Calcular el estadístico bootstrap (en este caso, la media de la muestra) 
usando la muestra bootstrap:


```{r, echo = F, out.width = "70%"}
mean1 <- round(mean(boot_samp_1$ppg), 3)
ggplot(data = boot_samp_1, aes(ppg)) +
  geom_histogram(binwidth = 25,
                 color = "darkblue",
                 fill = "skyblue") +
  labs(caption =  paste0("Bootstrap sample 1: mean = ", mean1),
       x = "Price per guest per night",
       y = "Count") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) + 
  geom_vline(xintercept = mean(boot_samp_1$ppg), lwd = 2, color = "red") + 
  theme_minimal()
```

---

## Paso a paso

**Paso 3.** Repetir los pasos 1 y 2 varias veces para crear una distribución bootstrap 
de medias de muestra:

.pull-left[
```{r, echo = F}
set.seed(1)
temp <- abb %>% slice(sample(1:50, replace = T))
samp1 <- ggplot(data = temp, aes(ppg)) +
  geom_histogram(binwidth = 25, color = "darkblue", fill = "skyblue") +
  labs(x = "", y = "") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(temp$ppg), lwd = 1, color = "red") + 
  theme_minimal()
```
```{r, echo = F}
set.seed(2)
temp <- abb %>% slice(sample(1:50, replace = T))
samp2 <- ggplot(data = temp, aes(ppg)) +
  geom_histogram(binwidth = 25, color = "darkblue", fill = "skyblue") +
  labs(x = "", y = "") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(temp$ppg), lwd = 1, color = "red") + 
  theme_minimal()
```
]

.pull-right[
```{r, echo = F}
set.seed(3)
temp <- abb %>% slice(sample(1:50, replace = T))
samp3 <- ggplot(data = temp, aes(ppg)) +
  geom_histogram(binwidth = 25, color = "darkblue", fill = "skyblue") +
  labs(x = "", y = "") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(temp$ppg), lwd = 1, color = "red") + 
  theme_minimal()
```

```{r, echo = F}
set.seed(4)
temp <- abb %>% slice(sample(1:50, replace = T))
samp4 <- ggplot(data = temp, aes(ppg)) +
  geom_histogram(binwidth = 25, color = "darkblue", fill = "skyblue") +
  labs(x = "", y = "") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(temp$ppg), lwd = 1, color = "red") + 
  theme_minimal()
```
]

```{r echo = F, out.width="80%"}
(samp1 + samp2)/(samp3 + samp4)
```

---

## Paso a paso

**Paso 3.** En este gráfico, hemos tomado 500 muestras bootstrap, calculado la
media de la muestra para cada una y las hemos representado en un histograma:

```{r, echo = F, out.width = "70%"}
boot_dist <- tibble(boot_mean = numeric(500))
for(i in 1:500){
  set.seed(i)
  indices <- sample(1:50, replace = T)
  boot_dist$boot_mean[i] <- mean(abb$ppg[indices])
}
ggplot(data = boot_dist, aes(boot_mean)) +
  geom_histogram(binwidth = 5, color = "darkred", fill = "pink") +
  labs(x = "", y = "", title = "Bootstrap distribution of sample means") +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  theme_minimal()
```


---

**Aquí comparamos la distribución bootstrap de las medias de la muestra con la distribución de los datos originales. ¿Qué observas?**

```{r, echo = F}
boot_dist <- tibble(boot_mean = numeric(500))
for(i in 1:500){
  set.seed(i)
  indices <- sample(1:50, replace = T)
  boot_dist$boot_mean[i] <- mean(abb$ppg[indices])
}
p1 <- ggplot(data = abb, aes(ppg)) +
  geom_histogram(binwidth = 25,
                 color = "darkblue",
                 fill = "skyblue") +
  labs(x = "", y = "", title = "Original sample") +
  scale_y_continuous(breaks = seq(0, 20, by = 5)) + 
  ylim(0, 20) +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = mean(abb$ppg), lwd = 2, color = "red") + 
  theme_minimal() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
p2 <- ggplot(data = boot_dist, aes(boot_mean)) +
  geom_histogram(binwidth = 5, color = "darkred", fill = "pink") +
  labs(x = "", y = "", title = "Bootstrap distribution of sample means") +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  theme_minimal() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
p1 / p2
```

---

## Paso a paso

**Paso 4.** Calcula los límites del intervalo bootstrap utilizando percentiles de la distribución bootstrap

```{r, echo = F, out.width = "70%"}
boot_dist <- tibble(boot_mean = numeric(500))
for(i in 1:500){
  set.seed(i)
  indices <- sample(1:50, replace = T)
  boot_dist$boot_mean[i] <- mean(abb$ppg[indices])
}
ggplot(data = boot_dist, aes(boot_mean)) +
  geom_histogram(binwidth = 5, color = "darkred", fill = "pink") +
  labs(x = "", y = "", title = "Bootstrap distribution of sample means",
       subtitle = "95% confidence interval bounds") +
  scale_x_continuous(breaks = seq(0, 250, by = 50)) + 
  xlim(0, 250) +
  geom_vline(xintercept = quantile(boot_dist$boot_mean, 0.025), 
             lwd = 1, color = "black") +
  geom_vline(xintercept = quantile(boot_dist$boot_mean, 0.975), 
             lwd = 1, color = "black") +
  theme_minimal()
```

---

class: center, middle

# Bootstrapping en R

---

## Paquete `infer`

.pull-left[
![](img/infer.png)
]

.pull-right[
<br/><br/><br/><br/>
El objetivo del paquete infer es realizar inferencia estadística utilizando una
gramática que se integra con el marco de diseño tidyverse.

```{r}
library(infer)
```

<br><br>
[infer.netlify.com](http://infer.netlify.com)
]

---

## Elige una semilla


```{r}
set.seed(123)
```

La función `set.seed()` es una función base de R que nos permite controlar la generación de números aleatorios en R. Úsala para hacer que tu simulación sea reproducible.

En otras palabras, asegura que obtendremos la misma muestra aleatoria cada vez que ejecutemos el código o compilemos.

---

## Generar medias bootstrap

```{r eval=FALSE}
abb %>%
  # specify the variable of interest
  specify(response = ppg) #<<
```

- `specify()` se utiliza para especificar qué variable en nuestro dataset es la
variable respuesta relevante (es decir, la variable que estamos sobre la que haremos
bootstrap).


---

## Generar medias bootstrap

```{r eval=FALSE}
abb %>%
  # specify the variable of interest
  specify(response = ppg) %>% 
  # generate 15000 bootstrap samples
  generate(reps = 15000, type = "bootstrap") #<<
```

- `generate()` generar las medias bootstrap

---

## Generar medias bootstrap

```{r eval=FALSE}
abb %>%
  # specify the variable of interest
  specify(response = ppg) %>% 
  # generate 15000 bootstrap samples
  generate(reps = 15000, type = "bootstrap") %>% 
  # calculate the statistic of each bootstrap sample
  calculate(stat = "mean") #<<
```

- `calculate()` calcula el estadístico muestra. En este caso usamos la media, pero podríamos calcular, por ejemplo,
las medianas bootstrap.


---

## Generar medias bootstrap

```{r include=FALSE}
set.seed(834782)
```

```{r}
num_reps <- 100
```

```{r}
# save resulting bootstrap distribution
boot_dist <- abb %>% #<<
  # specify the variable of interest
  specify(response = ppg) %>% 
  # generate 100 bootstrap samples
  generate(reps = num_reps, type = "bootstrap") %>% 
  # calculate the statistic of each bootstrap sample
  calculate(stat = "mean")
```

---

## Medias muestrales


¿Cuántas observaciones hay en `boot_dist`? ¿Qué representa cada observación?

--

```{r}
boot_dist
```

---

## Visualizando la distribución

```{r echo=FALSE}
ggplot(data = boot_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Distribución Bootstrap centrada alrededor de 75",
       x = "Precio por cliente", y = "Número", caption = "Binwidth of 5") +
  theme_minimal()
```

---

## Cálculo del intervalo de confianza

Un intervalo de confianza del 95% contiene el 95% central de la distribución bootstrap. Para obtener el 95% central, queremos omitir el 2.5% en la izquierda y en la derecha.


--

Usando `dplyr`:

```{r}
ci <- boot_dist %>%
  summarise(lower_b = quantile(stat, 0.025), 
            upper_b = quantile(stat, 0.975))
ci
```

---

## Visualizar un intervalo de confianza (Opción 1)

Utilizando `geom_vline()` para marcar los límites del intervalo de confianza


```{r eval = F, out.width = "75%"}
ggplot(data = boot_dist, mapping = aes(x = stat)) + #<<
  geom_histogram(binwidth = 5, alpha = .5) +
  geom_vline(xintercept = ci$lower_b, #<<
             color = "steelblue", lty = 2, size = 1) + 
  geom_vline(xintercept = ci$upper_b, #<<
             color = "steelblue", lty = 2, size = 1) +
  labs(title = "Distribución Bootstrap de la media de precio por cliente",
       subtitle = "con IC de 95%",
       x = "Medias", y = "Num") +
  theme_minimal()
```

---

## Visualizar un intervalo de confianza (Opción 1)


```{r echo=FALSE, out.width = "75%"}
ggplot(data = boot_dist, mapping = aes(x = stat)) + #<<
  geom_histogram(binwidth = 5, alpha = .5) +
  geom_vline(xintercept = ci$lower_b, #<<
             color = "steelblue", lty = 2, size = 1) + 
  geom_vline(xintercept = ci$upper_b, #<<
             color = "steelblue", lty = 2, size = 1) +
  labs(title = "Distribución Bootstrap de la media de precio por cliente",
       subtitle = "con IC de 95%",
       x = "Medias", y = "Num") +
  theme_minimal()
```

---

## Visualizar un intervalo de confianza (Opción 2)

```{r eval = F, out.width = "75%"}
visualize(boot_dist) + #<<
  shade_confidence_interval(endpoints = ci)+ #<<
  labs(title = "Distribución Bootstrap de la media de precio por cliente",
       subtitle = "con IC de 95%",
       x = "Medias", y = "Num") 
```
---

## Visualizar un intervalo de confianza (Opción 2)

```{r echo = F, out.width = "75%"}
visualize(boot_dist) + #<<
  shade_confidence_interval(endpoints = ci)+ #<<
  labs(title = "Distribución Bootstrap de la media de precio por cliente",
       subtitle = "con IC de 95%",
       x = "Medias", y = "Num") 
```
---


## Interpretación de un intervalo de confianza

Nuestro intervalo de confianza del 95% (IC) es 
(`r round(ci$lower_b, 1)`, `r round(ci$upper_b, 1)`).

.question[
¿Significa esto que hay un 95% de probabilidad de que la media del precio por noche en la población esté contenida en el intervalo (`r round(ci$lower_b, 1)`, `r round(ci$upper_b, 1)`)? 
]

---

class: center, middle

.question[
**¡No!**
]

---

## Interpretación de un intervalo de confianza

- El parámetro de la población está o no está en nuestro intervalo. No puede tener una "probabilidad del 95%" de estar en algún intervalo específico.

--

- La distribución bootstrap captura la variabilidad de la media de la muestra, pero se basa en nuestra muestra original. Si comenzáramos con una muestra diferente, tal vez nuestro intervalo de confianza estimado del 95%  fuese diferente.

--

- Todo lo que podemos decir es que, si tomáramos muestras repetidas de manera independiente de esta población y calculáramos un IC del 95% para la media de la misma manera, entonces *esperaríamos* que el 95% de estos intervalos contuvieran la media de la población.

--

- Sin embargo, ¡nunca sabemos si algún intervalo en particular realmente lo hace!

---

## Interpretación 

.question[
**Tenemos un 95% de confianza en que el precio medio por huésped por noche para Airbnbs en Asheville, NC está entre `r round(ci$lower_b,1)` y `r round(ci$upper_b,1)` dólares.**
]

---
## Bibliografía

Este tema está fundamentalmente basado en  [R for Data Science](https://r4ds.had.co.nz/), Wickham and Grolemund (2016)